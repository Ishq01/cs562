{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d26328",
   "metadata": {},
   "source": [
    "1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d761f38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "PyTorch compiled with CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"PyTorch compiled with CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16089774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thejo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda with microsoft/deberta-v3-base\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "RoBERTa QA with Differential Privacy (Opacus)\n",
    "Optimized for CS 562\n",
    "\"\"\"\n",
    "\n",
    "# 1. Install necessary libraries\n",
    "import sys\n",
    "# Note: You can comment out the pip install line if libraries are already installed\n",
    "# !pip -q install transformers datasets accelerate opacus pandas matplotlib tqdm scikit-learn sentencepiece\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# Import AdamW from PyTorch directly\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForQuestionAnswering, \n",
    "    # AdamW,  <-- REMOVED from here\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "class Config:\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-base\" \n",
    "    \n",
    "    # CHANGED: 384 is standard and MUCH faster than 512\n",
    "    MAX_LENGTH = 384\n",
    "    DOC_STRIDE = 128\n",
    "    \n",
    "    # 8 is fine, but if you have VRAM issues, drop to 4\n",
    "    BATCH_SIZE = 8\n",
    "    GRAD_ACCUM_STEPS = 4\n",
    "    \n",
    "    LEARNING_RATE = 2.5e-5\n",
    "    \n",
    "    # CHANGED: With the full dataset, 2 epochs is enough for convergence\n",
    "    EPOCHS = 2\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    SEED = 42\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # DP Specifics\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    DELTA = 1e-5\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(config.SEED)\n",
    "print(f\"Running on {config.DEVICE} with {config.MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e07ec",
   "metadata": {},
   "source": [
    "2. Data Cleaning & Tokenization (Robust Version)\n",
    "\n",
    "I cleaned up the concatenation logic to be more robust against HotpotQA's weird formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0774298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thejo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Cleaning Text Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Data: 100%|██████████| 90447/90447 [00:12<00:00, 7105.69it/s]\n",
      "Cleaning Data: 100%|██████████| 2000/2000 [00:00<00:00, 6769.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Converting to HF Datasets (Train: 84933)...\n",
      "3. Tokenizing & Caching (Speed Optimization)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Train (Dropping Empty Windows): 100%|██████████| 84933/84933 [01:33<00:00, 908.80 examples/s]\n",
      "Processing Val: 100%|██████████| 1875/1875 [00:02<00:00, 894.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Optimized Training Features: 114291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 4: FAST DATA PROCESSING (Aggressive Filtering) ---\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, use_fast=True)\n",
    "\n",
    "def clean_hotpot_data(dataset_split, max_examples=None):\n",
    "    # (This function stays the same as before)\n",
    "    records = []\n",
    "    indices = range(len(dataset_split))\n",
    "    if max_examples:\n",
    "        indices = indices[:max_examples]\n",
    "\n",
    "    for i in tqdm(indices, desc=\"Cleaning Data\"):\n",
    "        ex = dataset_split[i]\n",
    "        ans = ex[\"answer\"]\n",
    "        if not ans or ans.lower() in [\"yes\", \"no\"]:\n",
    "            continue\n",
    "        context_str = \"\"\n",
    "        for item in ex[\"context\"][\"sentences\"]:\n",
    "             paragraph = \" \".join(item)\n",
    "             context_str += paragraph + \" \"\n",
    "        context_str = context_str.strip()\n",
    "        start_idx = context_str.lower().find(ans.lower())\n",
    "        if start_idx == -1: continue\n",
    "        end_idx = start_idx + len(ans)\n",
    "        records.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"question\": ex[\"question\"],\n",
    "            \"context\": context_str,\n",
    "            \"answer_text\": ans,\n",
    "            \"answer_start\": start_idx,\n",
    "            \"answer_end\": end_idx\n",
    "        })\n",
    "    return records\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        stride=config.DOC_STRIDE,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    out = {\"input_ids\": [], \"attention_mask\": [], \"start_positions\": [], \"end_positions\": [], \"example_id\": []}\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        start_char = examples[\"answer_start\"][sample_index]\n",
    "        end_char = examples[\"answer_end\"][sample_index]\n",
    "        ex_id = examples[\"id\"][sample_index]\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1: idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        is_answer_in_context = (offsets[context_start][0] <= start_char and offsets[context_end][1] >= end_char)\n",
    "        \n",
    "        # --- SPEED FIX IS HERE ---\n",
    "        # If the answer is NOT in this window, DROP IT IMMEDIATELY.\n",
    "        # We don't train on empty windows. This cuts the dataset size in half.\n",
    "        if not is_answer_in_context:\n",
    "            continue\n",
    "            \n",
    "        # Calculate positions\n",
    "        idx_start = context_start\n",
    "        while idx_start <= context_end and offsets[idx_start][0] <= start_char: idx_start += 1\n",
    "        s_pos = idx_start - 1\n",
    "        idx_end = context_end\n",
    "        while idx_end >= context_start and offsets[idx_end][1] >= end_char: idx_end -= 1\n",
    "        e_pos = idx_end + 1\n",
    "\n",
    "        out[\"input_ids\"].append(tokenized[\"input_ids\"][i])\n",
    "        out[\"attention_mask\"].append(tokenized[\"attention_mask\"][i])\n",
    "        out[\"start_positions\"].append(s_pos)\n",
    "        out[\"end_positions\"].append(e_pos)\n",
    "        out[\"example_id\"].append(ex_id)\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "raw_dataset = load_dataset(\"hotpot_qa\", \"distractor\")\n",
    "\n",
    "print(\"1. Cleaning Text Data...\")\n",
    "train_records = clean_hotpot_data(raw_dataset[\"train\"], max_examples=None) # Full Dataset\n",
    "val_records = clean_hotpot_data(raw_dataset[\"validation\"], max_examples=2000)\n",
    "\n",
    "print(f\"2. Converting to HF Datasets (Train: {len(train_records)})...\")\n",
    "train_hf = HFDataset.from_list(train_records)\n",
    "val_hf = HFDataset.from_list(val_records)\n",
    "\n",
    "print(\"3. Tokenizing & Caching (Speed Optimization)...\")\n",
    "# Note: We use prepare_train_features for BOTH now to enforce the filtering\n",
    "train_ds = train_hf.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=train_hf.column_names,\n",
    "    desc=\"Processing Train (Dropping Empty Windows)\"\n",
    ")\n",
    "\n",
    "# For validation, we usually want to keep some context, but for pure speed\n",
    "# we can use the same function.\n",
    "val_ds = val_hf.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=val_hf.column_names,\n",
    "    desc=\"Processing Val\"\n",
    ")\n",
    "\n",
    "train_ds.set_format(\"torch\")\n",
    "val_ds.set_format(\"torch\")\n",
    "\n",
    "print(f\"Final Optimized Training Features: {len(train_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df39d1",
   "metadata": {},
   "source": [
    "3. Metric Calculation (F1 / EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547b6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace.\"\"\"\n",
    "    import string, re\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def compute_exact(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def evaluate_model(model, eval_dataset, dataset_clean):\n",
    "    device = config.DEVICE\n",
    "    model.eval()\n",
    "    \n",
    "    id_to_truth = {ex[\"id\"]: ex[\"answer_text\"] for ex in dataset_clean}\n",
    "    all_predictions = {}\n",
    "    \n",
    "    # FIX 1: Use the passed dataset directly (no QADataset wrapper needed)\n",
    "    eval_loader = DataLoader(eval_dataset, batch_size=config.BATCH_SIZE * 2, shuffle=False)\n",
    "    \n",
    "    # FIX 2: Pre-load Example IDs list because DataLoader drops string columns when using torch format\n",
    "    eval_example_ids = eval_dataset[\"example_id\"]\n",
    "    \n",
    "    feature_idx = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            start_logits = outputs.start_logits.cpu().numpy()\n",
    "            end_logits = outputs.end_logits.cpu().numpy()\n",
    "            \n",
    "            for i in range(len(input_ids)):\n",
    "                # FIX 3: Get ID from our pre-loaded list using the global index\n",
    "                ex_id = eval_example_ids[feature_idx]\n",
    "                feature_idx += 1\n",
    "                \n",
    "                start_log = start_logits[i]\n",
    "                end_log = end_logits[i]\n",
    "                \n",
    "                s_idx = np.argmax(start_log)\n",
    "                e_idx = np.argmax(end_log)\n",
    "                \n",
    "                if e_idx < s_idx or (e_idx - s_idx) > 30:\n",
    "                     e_idx = s_idx\n",
    "                \n",
    "                score = start_log[s_idx] + end_log[e_idx]\n",
    "                pred_ids = input_ids[i][s_idx : e_idx + 1]\n",
    "                pred_text = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "                \n",
    "                if ex_id not in all_predictions or score > all_predictions[ex_id][0]:\n",
    "                    all_predictions[ex_id] = (score, pred_text)\n",
    "\n",
    "    f1s, ems = [], []\n",
    "    for ex_id, (_, pred_text) in all_predictions.items():\n",
    "        if ex_id in id_to_truth:\n",
    "            truth = id_to_truth[ex_id]\n",
    "            f1s.append(compute_f1(pred_text, truth))\n",
    "            ems.append(compute_exact(pred_text, truth))\n",
    "        \n",
    "    return np.mean(ems), np.mean(f1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a51bda",
   "metadata": {},
   "source": [
    "4. Baseline Training (Non-DP)\n",
    "\n",
    "This is the \"standard\" training to prove the model works before adding noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97d153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Optimized Base Model on cuda ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 14287/14287 [1:32:02<00:00,  2.59it/s, loss=2.63] \n",
      "Evaluating: 100%|██████████| 157/157 [01:23<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | EM: 0.6133 | F1: 0.7378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 14287/14287 [1:32:09<00:00,  2.58it/s, loss=2.07] \n",
      "Evaluating: 100%|██████████| 157/157 [01:23<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | EM: 0.6347 | F1: 0.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_optimized_base():\n",
    "    print(f\"\\n=== Training Optimized Base Model on {config.DEVICE} ===\")\n",
    "    \n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        config.MODEL_NAME, \n",
    "        use_safetensors=True\n",
    "    ).to(config.DEVICE)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    # PASS DATASET DIRECTLY (train_ds is now the HF dataset)\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    \n",
    "    total_steps = (len(train_loader) // config.GRAD_ACCUM_STEPS) * config.EPOCHS\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps), \n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    use_cuda = (config.DEVICE == \"cuda\")\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=use_cuda)\n",
    "    \n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        current_loss = 0\n",
    "        \n",
    "        for step, batch in enumerate(loop):\n",
    "            input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "            start_positions = batch['start_positions'].to(config.DEVICE)\n",
    "            end_positions = batch['end_positions'].to(config.DEVICE)\n",
    "            \n",
    "            with torch.amp.autocast(device_type=config.DEVICE, dtype=torch.float16, enabled=use_cuda):\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                                start_positions=start_positions, end_positions=end_positions)\n",
    "                loss = outputs.loss / config.GRAD_ACCUM_STEPS\n",
    "\n",
    "            if use_cuda:\n",
    "                scaler.scale(loss).backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            current_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % config.GRAD_ACCUM_STEPS == 0:\n",
    "                if use_cuda:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                \n",
    "                loop.set_postfix(loss=current_loss * config.GRAD_ACCUM_STEPS)\n",
    "                current_loss = 0\n",
    "\n",
    "        # FIX: Pass val_ds (the dataset) and val_records (the ground truth list)\n",
    "        em, f1 = evaluate_model(model, val_ds, val_records) \n",
    "        print(f\"Epoch {epoch+1} | EM: {em:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run it!\n",
    "baseline_model = train_optimized_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252cc6a",
   "metadata": {},
   "source": [
    "5. DP-SGD Training with Opacus\n",
    "\n",
    "Key Improvement: I added ModuleValidator.fix(model). RoBERTa has components that might not be DP-friendly. The validator replaces them with DP-compliant versions (e.g., replacing standard BatchNorm with GroupNorm if it existed, though RoBERTa is mostly LayerNorm which is fine, but it's safer to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_private(target_epsilon, target_delta, sigma=1.0, max_grad_norm=1.0):\n",
    "    print(f\"\\n=== Training Private (DP) | Target Epsilon: {target_epsilon} ===\")\n",
    "    \n",
    "    # 1. Initialize Model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # 2. Fix layers for DP compatibility (CRITICAL STEP)\n",
    "    # This replaces layers that Opacus doesn't support with compliant ones\n",
    "    model = ModuleValidator.fix(model)\n",
    "    model = model.to(config.DEVICE)\n",
    "    model.train() # Must be in train mode for Opacus attachment\n",
    "    \n",
    "    # 3. Optimizer (Standard AdamW)\n",
    "    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n",
    "    \n",
    "    # 4. DataLoader\n",
    "    # Opacus works best when it wraps the dataloader\n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 5. Attach Privacy Engine\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    \n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=sigma,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "    )\n",
    "    \n",
    "    print(f\"DP Engine Attached. Noise: {sigma}, Clipping: {max_grad_norm}\")\n",
    "    \n",
    "    # Scheduler (Must be created AFTER make_private for correct step calculation)\n",
    "    # Note: Opacus changes the concept of 'steps' due to Poisson sampling, \n",
    "    # but linear decay is still fine.\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)\n",
    "\n",
    "    # 6. Training Loop\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        # BatchMemoryManager prevents OOM by virtually splitting batches\n",
    "        # while keeping the mathematical definition of the DP batch size intact.\n",
    "        with BatchMemoryManager(\n",
    "            data_loader=train_loader, \n",
    "            max_physical_batch_size=8,  # Fit into GPU memory\n",
    "            optimizer=optimizer\n",
    "        ) as memory_safe_loader:\n",
    "            \n",
    "            loop = tqdm(memory_safe_loader, desc=f\"DP Epoch {epoch+1}\")\n",
    "            \n",
    "            for batch in loop:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "                start_positions = batch['start_positions'].to(config.DEVICE)\n",
    "                end_positions = batch['end_positions'].to(config.DEVICE)\n",
    "                \n",
    "                # Note: Mixed Precision (AMP) is tricky with Opacus in older versions.\n",
    "                # Usually better to run FP32 for DP unless using latest Opacus + Pytorch.\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                                start_positions=start_positions, end_positions=end_positions)\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        # Check Privacy Budget\n",
    "        epsilon = privacy_engine.get_epsilon(target_delta)\n",
    "        print(f\"Epoch {epoch+1} | Privacy Budget Spent: ε = {epsilon:.2f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        # Note: We must use model._module or model to access the underlying HF model for eval\n",
    "        # Opacus wraps the model, so we evaluate carefully.\n",
    "        em, f1 = evaluate_model(model, val_features, val_clean)\n",
    "        print(f\"Epoch {epoch+1} | EM: {em:.4f} | F1: {f1:.4f}\")\n",
    "        \n",
    "        if epsilon > target_epsilon:\n",
    "            print(\"Target Epsilon exceeded. Stopping.\")\n",
    "            break\n",
    "\n",
    "# Run the DP Experiment\n",
    "# Sigma (noise) and Max Grad Norm are hyperparameters you tune.\n",
    "# Higher Sigma = More Privacy (lower Epsilon) but worse Accuracy.\n",
    "train_private(target_epsilon=10, target_delta=1e-5, sigma=1.0, max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25247ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
